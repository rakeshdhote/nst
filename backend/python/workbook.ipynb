{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob as glob\n",
    "import os\n",
    "import json\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "import ollama\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('.env.local') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ollama model list\n",
    "from datetime import datetime\n",
    "\n",
    "# Assuming the ListResponse and related classes are defined as follows:\n",
    "class ModelDetails:\n",
    "    def __init__(self, parent_model, format, family, families, parameter_size, quantization_level):\n",
    "        self.parent_model = parent_model\n",
    "        self.format = format\n",
    "        self.family = family\n",
    "        self.families = families\n",
    "        self.parameter_size = parameter_size\n",
    "        self.quantization_level = quantization_level\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, model, modified_at, digest, size, details):\n",
    "        self.model = model\n",
    "        self.modified_at = modified_at\n",
    "        self.digest = digest\n",
    "        self.size = size\n",
    "        self.details = details\n",
    "\n",
    "class ListResponse:\n",
    "    def __init__(self, models):\n",
    "        self.models = models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ListResponse(models=[Model(model='smollm2:135m', modified_at=datetime.datetime(2024, 12, 18, 11, 49, 6, 802093, tzinfo=TzInfo(-05:00)), digest='9077fe9d2ae1a4a41a868836b56b8163731a8fe16621397028c2c76f838c6907', size=270898672, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='134.52M', quantization_level='F16')), Model(model='llava:latest', modified_at=datetime.datetime(2024, 12, 7, 23, 46, 35, 567979, tzinfo=TzInfo(-05:00)), digest='8dd30f6b0cb19f555f2c7a7ebda861449ea2cc76bf1f44e262931f45fc81d081', size=4733363377, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama', 'clip'], parameter_size='7B', quantization_level='Q4_0')), Model(model='moondream:latest', modified_at=datetime.datetime(2024, 12, 7, 22, 19, 46, 414440, tzinfo=TzInfo(-05:00)), digest='55fc3abd386771e5b5d1bbcc732f3c3f4df6e9f9f08f1131f9cc27ba2d1eec5b', size=1738451197, details=ModelDetails(parent_model='', format='gguf', family='phi2', families=['phi2', 'clip'], parameter_size='1B', quantization_level='Q4_0')), Model(model='llama3.2:latest', modified_at=datetime.datetime(2024, 12, 7, 22, 18, 36, 845215, tzinfo=TzInfo(-05:00)), digest='a80c4f17acd55265feec403c7aef86be0c25983ab279d83f3bcd3abbcb5b8b72', size=2019393189, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='3.2B', quantization_level='Q4_K_M'))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: smollm2:135m\n",
      "Modified At: 2024-12-18 11:49:06.802093-05:00\n",
      "Digest: 9077fe9d2ae1a4a41a868836b56b8163731a8fe16621397028c2c76f838c6907\n",
      "Size: 270898672 bytes\n",
      "Parameter Size: 134.52M\n",
      "Quantization Level: F16\n",
      "Family: llama\n",
      "Families: llama\n",
      "----------------------------------------\n",
      "Model Name: llava:latest\n",
      "Modified At: 2024-12-07 23:46:35.567979-05:00\n",
      "Digest: 8dd30f6b0cb19f555f2c7a7ebda861449ea2cc76bf1f44e262931f45fc81d081\n",
      "Size: 4733363377 bytes\n",
      "Parameter Size: 7B\n",
      "Quantization Level: Q4_0\n",
      "Family: llama\n",
      "Families: llama, clip\n",
      "----------------------------------------\n",
      "Model Name: moondream:latest\n",
      "Modified At: 2024-12-07 22:19:46.414440-05:00\n",
      "Digest: 55fc3abd386771e5b5d1bbcc732f3c3f4df6e9f9f08f1131f9cc27ba2d1eec5b\n",
      "Size: 1738451197 bytes\n",
      "Parameter Size: 1B\n",
      "Quantization Level: Q4_0\n",
      "Family: phi2\n",
      "Families: phi2, clip\n",
      "----------------------------------------\n",
      "Model Name: llama3.2:latest\n",
      "Modified At: 2024-12-07 22:18:36.845215-05:00\n",
      "Digest: a80c4f17acd55265feec403c7aef86be0c25983ab279d83f3bcd3abbcb5b8b72\n",
      "Size: 2019393189 bytes\n",
      "Parameter Size: 3.2B\n",
      "Quantization Level: Q4_K_M\n",
      "Family: llama\n",
      "Families: llama\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ollama_models = ollama.list()\n",
    "\n",
    "for model in ollama_models.models:\n",
    "    print(f\"Model Name: {model.model}\")\n",
    "    print(f\"Modified At: {model.modified_at}\")\n",
    "    print(f\"Digest: {model.digest}\")\n",
    "    print(f\"Size: {model.size} bytes\")\n",
    "    print(f\"Parameter Size: {model.details.parameter_size}\")\n",
    "    print(f\"Quantization Level: {model.details.quantization_level}\")\n",
    "    print(f\"Family: {model.details.family}\")\n",
    "    print(f\"Families: {', '.join(model.details.families)}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from litellm import completion, success_callback\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from pprint import pprint\n",
    "\n",
    "# Global cost tracker\n",
    "COST_TRACKER = {\"cost\": 0.0}\n",
    "\n",
    "# track_cost_callback\n",
    "def track_cost_callback(kwargs, completion_response, start_time, end_time, stream=False):\n",
    "    # Extract usage from the response if available and calculate cost.\n",
    "    # This calculation is just an example; the actual cost logic depends on your provider/pricing model.\n",
    "    try:\n",
    "        # Try converting the response to a dict\n",
    "        if hasattr(completion_response, \"to_dict\"):\n",
    "            response_dict = completion_response.to_dict()\n",
    "        else:\n",
    "            response_dict = json.loads(str(completion_response))\n",
    "        \n",
    "        usage = response_dict.get(\"usage\", {})\n",
    "        # Example: If each 1K tokens cost $0.003:\n",
    "        total_tokens = usage.get(\"total_tokens\", 0)\n",
    "        COST_TRACKER[\"cost\"] = (total_tokens / 1000.0) * 0.003\n",
    "        \n",
    "        print(\"Calculated cost:\", COST_TRACKER[\"cost\"])\n",
    "    except Exception as e:\n",
    "        print(\"Error in track_cost_callback:\", e)\n",
    "\n",
    "# Set the success callback\n",
    "success_callback[:] = [track_cost_callback]\n",
    "\n",
    "#######################################################\n",
    "# def create_file_tree(\n",
    "#     summaries: list,\n",
    "#     host: str,\n",
    "#     port: int,\n",
    "#     model: str = \"llama-3.1-70b-versatile\",\n",
    "#     api_base: str = None,\n",
    "#     stream: bool = False\n",
    "# ) -> list:\n",
    "#     \"\"\"\n",
    "#     Create a file tree based on the provided summaries.\n",
    "\n",
    "#     Args:\n",
    "#         summaries (list): List of file summaries to organize.\n",
    "#         host (str): Host address for the API.\n",
    "#         port (int): Port number for the API.\n",
    "#         model (str, optional): AI model to use for file organization. \n",
    "#                                Defaults to \"llama-3.1-70b-versatile\".\n",
    "#         api_base (str, optional): Base URL for the API. If not provided, constructed from host and port.\n",
    "#         stream (bool, optional): Whether to use streaming. Defaults to False.\n",
    "\n",
    "#     Returns:\n",
    "#         list: Reorganized file paths.\n",
    "#     \"\"\"\n",
    "#     PROMPT = \"\"\"\n",
    "#     You will be provided with a list of source files and a summary of their contents. For each file, propose a new path and filename, using a directory structure that optimally organizes the files using known conventions and best practices.\n",
    "#     Follow good naming conventions. Here are a few guidelines:\n",
    "#     - Think about your files: What related files are you working with?\n",
    "#     - Identify metadata (for example, date, sample, experiment): What information is needed to easily locate a specific file?\n",
    "#     - Abbreviate or encode metadata\n",
    "#     - Use versioning: Are you maintaining different versions of the same file?\n",
    "#     - Think about how you will search for your files: What comes first?\n",
    "#     - Deliberately separate metadata elements: Avoid spaces or special characters in your file names\n",
    "#     If the file is already named well or matches a known convention, set the destination path to the same as the source path.\n",
    "\n",
    "#     Your response must be a JSON object with the following schema:\n",
    "#     ```json\n",
    "#     {\n",
    "#         \"files\": [\n",
    "#             {\n",
    "#                 \"src_path\": \"original file path\",\n",
    "#                 \"dst_path\": \"new file path under proposed directory structure with proposed file name\"\n",
    "#             }\n",
    "#         ]\n",
    "#     }\n",
    "#     ```\n",
    "#     \"\"\".strip()\n",
    "\n",
    "#     if not api_base:\n",
    "#         api_base = f\"http://{host}:{port}\"\n",
    "\n",
    "#     try:\n",
    "#         response = completion(\n",
    "#             model=model,\n",
    "#             messages=[\n",
    "#                 {\"role\": \"system\", \"content\": PROMPT},\n",
    "#                 {\"role\": \"user\", \"content\": json.dumps(summaries)},\n",
    "#             ],\n",
    "#             response_format={\"type\": \"json_object\"},\n",
    "#             temperature=0,\n",
    "#             api_base=api_base,\n",
    "#             stream=stream\n",
    "#         )\n",
    "#     except Exception as e:\n",
    "#         print(f\"LiteLLM Error >>> {e}\")\n",
    "#         return []\n",
    "\n",
    "#     if response is None:\n",
    "#         print(\"No response received from the API.\")\n",
    "#         return []\n",
    "\n",
    "#     # Convert the response to a dict\n",
    "#     try:\n",
    "#         if hasattr(response, \"to_dict\"):\n",
    "#             response_dict = response.to_dict()\n",
    "#         else:\n",
    "#             response_dict = json.loads(str(response))\n",
    "#     except (TypeError, json.JSONDecodeError) as e:\n",
    "#         print(f\"Error parsing response: {e}\")\n",
    "#         return []\n",
    "\n",
    "#     content = response_dict.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "\n",
    "#     # Attempt to parse the JSON content\n",
    "#     try:\n",
    "#         file_tree = json.loads(content)[\"files\"]\n",
    "#     except json.JSONDecodeError as e:\n",
    "#         print(f\"Error decoding JSON content: {e}\")\n",
    "#         return []\n",
    "#     except KeyError:\n",
    "#         print(\"The expected 'files' key is missing in the response.\")\n",
    "#         return []\n",
    "\n",
    "#     # Optional: Pretty-print the file tree for debugging\n",
    "#     # print(\"File Tree: >>>>>>>>>>> \")\n",
    "#     # pprint(file_tree)\n",
    "\n",
    "#     return file_tree\n",
    "\n",
    "#######################################################\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from pprint import pprint\n",
    "\n",
    "def create_file_tree(\n",
    "    summaries: List[Dict[str, Any]],\n",
    "    host: str,\n",
    "    port: int,\n",
    "    model: str = \"llama-3.1-70b-versatile\",\n",
    "    api_base: str = None,\n",
    "    stream: bool = False\n",
    ") -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Create a file tree based on the provided summaries.\n",
    "\n",
    "    Args:\n",
    "        summaries (list): List of file summaries to organize.\n",
    "        host (str): Host address for the API.\n",
    "        port (int): Port number for the API.\n",
    "        model (str, optional): AI model to use for file organization. \n",
    "                               Defaults to \"llama-3.1-70b-versatile\".\n",
    "        api_base (str, optional): Base URL for the API. If not provided, constructed from host and port.\n",
    "        stream (bool, optional): Whether to use streaming. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        list: Reorganized file paths.\n",
    "    \"\"\"\n",
    "    PROMPT = \"\"\"\n",
    "    You will be provided with a list of source files and a summary of their contents. For each file, propose a new path and filename, using a directory structure that optimally organizes the files using known conventions and best practices.\n",
    "    Follow good naming conventions. Here are a few guidelines:\n",
    "    - Think about your files: What related files are you working with?\n",
    "    - Identify metadata (for example, date, sample, experiment): What information is needed to easily locate a specific file?\n",
    "    - Abbreviate or encode metadata\n",
    "    - Use versioning: Are you maintaining different versions of the same file?\n",
    "    - Think about how you will search for your files: What comes first?\n",
    "    - Deliberately separate metadata elements: Avoid spaces or special characters in your file names\n",
    "    - Do not change the file extension\n",
    "    If the file is already named well or matches a known convention, set the destination path to the same as the source path.\n",
    "\n",
    "    Your response must be a JSON object with the following schema:\n",
    "    ```json\n",
    "    {\n",
    "        \"files\": [\n",
    "            {\n",
    "                \"src_path\": \"original file path\",\n",
    "                \"dst_path\": \"new file path under proposed directory structure with proposed file name\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    ```\n",
    "    \"\"\".strip()\n",
    "\n",
    "    if not api_base:\n",
    "        api_base = f\"http://{host}:{port}\"\n",
    "        print(\">>> api_base: \", api_base)\n",
    "\n",
    "    try:\n",
    "        response = completion(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": PROMPT},\n",
    "                {\"role\": \"user\", \"content\": json.dumps(summaries)},\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            # temperature=0,\n",
    "            api_base=api_base,\n",
    "            stream=stream\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"LiteLLM Error >>> {e}\")\n",
    "        return []\n",
    "\n",
    "    if response is None:\n",
    "        print(\"No response received from the API.\")\n",
    "        return []\n",
    "\n",
    "    # Convert the response to a dict\n",
    "    try:\n",
    "        if hasattr(response, \"to_dict\"):\n",
    "            response_dict = response.to_dict()\n",
    "        else:\n",
    "            response_dict = json.loads(str(response))\n",
    "    except (TypeError, json.JSONDecodeError) as e:\n",
    "        print(f\"Error parsing response: {e}\")\n",
    "        return []\n",
    "\n",
    "    content = response_dict.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "\n",
    "    # Attempt to parse the JSON content\n",
    "    try:\n",
    "        file_tree = json.loads(content)[\"files\"]\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON content: {e}\")\n",
    "        return []\n",
    "    except KeyError:\n",
    "        print(\"The expected 'files' key is missing in the response.\")\n",
    "        return []\n",
    "\n",
    "    return file_tree\n",
    "\n",
    "#######################################################\n",
    "# import json\n",
    "# from typing import List, Dict, Any\n",
    "# from pprint import pprint\n",
    "# from litellm import completion  # Ensure you have litellm imported correctly\n",
    "\n",
    "# def create_file_tree(\n",
    "#     summaries: List[Dict[str, Any]],\n",
    "#     host: str,\n",
    "#     port: int,\n",
    "#     model: str = \"llama-3.1-70b-versatile\",\n",
    "#     api_base: str = None,\n",
    "#     stream: bool = False\n",
    "# ) -> List[Dict[str, str]]:\n",
    "#     \"\"\"\n",
    "#     Create a file tree based on the provided summaries.\n",
    "\n",
    "#     Args:\n",
    "#         summaries (list): List of file summaries to organize.\n",
    "#         host (str): Host address for the API.\n",
    "#         port (int): Port number for the API.\n",
    "#         model (str, optional): AI model to use for file organization. \n",
    "#                                Defaults to \"llama-3.1-70b-versatile\".\n",
    "#         api_base (str, optional): Base URL for the API. If not provided, constructed from host and port.\n",
    "#         stream (bool, optional): Whether to use streaming. Defaults to False.\n",
    "\n",
    "#     Returns:\n",
    "#         list: Reorganized file paths.\n",
    "#     \"\"\"\n",
    "#     PROMPT = \"\"\"\n",
    "#     You will be provided with a list of source files and a summary of their contents. For each file, propose a new path and filename, using a directory structure that optimally organizes the files using known conventions and best practices.\n",
    "#     Follow good naming conventions. Here are a few guidelines:\n",
    "#     - Think about your files: What related files are you working with?\n",
    "#     - Identify metadata (for example, date, sample, experiment): What information is needed to easily locate a specific file?\n",
    "#     - Abbreviate or encode metadata\n",
    "#     - Use versioning: Are you maintaining different versions of the same file?\n",
    "#     - Think about how you will search for your files: What comes first?\n",
    "#     - Deliberately separate metadata elements: Avoid spaces or special characters in your file names\n",
    "#     If the file is already named well or matches a known convention, set the destination path to the same as the source path.\n",
    "\n",
    "#     Your response must be a JSON object with the following schema:\n",
    "#     ```json\n",
    "#     {\n",
    "#         \"files\": [\n",
    "#             {\n",
    "#                 \"src_path\": \"original file path\",\n",
    "#                 \"dst_path\": \"new file path under proposed directory structure with proposed file name\"\n",
    "#             }\n",
    "#         ]\n",
    "#     }\n",
    "#     ```\n",
    "#     \"\"\".strip()\n",
    "\n",
    "#     if not api_base:\n",
    "#         api_base = f\"http://{host}:{port}\"\n",
    "#         print(\">>> api_base: \", api_base)\n",
    "\n",
    "#     try:\n",
    "#         response = completion(\n",
    "#             model=model,\n",
    "#             messages=[\n",
    "#                 {\"role\": \"system\", \"content\": PROMPT},\n",
    "#                 {\"role\": \"user\", \"content\": json.dumps(summaries)},\n",
    "#             ],\n",
    "#             response_format=\"json\",  # Set to 'json' to receive a JSON object\n",
    "#             functions=None,          # Disable function calling\n",
    "#             api_base=api_base,\n",
    "#             stream=stream\n",
    "#         )\n",
    "#     except Exception as e:\n",
    "#         print(f\"LiteLLM Error >>> {e}\")\n",
    "#         return []\n",
    "\n",
    "#     if response is None:\n",
    "#         print(\"No response received from the API.\")\n",
    "#         return []\n",
    "\n",
    "#     # Convert the response to a dict\n",
    "#     try:\n",
    "#         if hasattr(response, \"to_dict\"):\n",
    "#             response_dict = response.to_dict()\n",
    "#         else:\n",
    "#             response_dict = json.loads(str(response))\n",
    "#     except (TypeError, json.JSONDecodeError) as e:\n",
    "#         print(f\"Error parsing response: {e}\")\n",
    "#         return []\n",
    "\n",
    "#     content = response_dict.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "\n",
    "#     # Attempt to parse the JSON content\n",
    "#     try:\n",
    "#         file_tree = json.loads(content)[\"files\"]\n",
    "#     except json.JSONDecodeError as e:\n",
    "#         print(f\"Error decoding JSON content: {e}\")\n",
    "#         return []\n",
    "#     except KeyError:\n",
    "#         print(\"The expected 'files' key is missing in the response.\")\n",
    "#         return []\n",
    "\n",
    "#     return file_tree\n",
    "\n",
    "#####################################################\n",
    "def load_documents(path: str) -> list:\n",
    "    \"\"\"\n",
    "    Load documents from the specified path.\n",
    "\n",
    "    Args:\n",
    "        path (str): Directory path to load documents from.\n",
    "\n",
    "    Returns:\n",
    "        list: List of document dictionaries containing content and metadata.\n",
    "    \"\"\"\n",
    "    reader = SimpleDirectoryReader(input_dir=path)\n",
    "    documents = reader.load_data()\n",
    "    doc_dicts = [{\"content\": d.text, **d.metadata} for d in documents]\n",
    "    return doc_dicts\n",
    "\n",
    "def process_metadata(doc_dicts: list) -> list:\n",
    "    \"\"\"\n",
    "    Process metadata to remove duplicate file entries.\n",
    "\n",
    "    Args:\n",
    "        doc_dicts (list): List of document dictionaries.\n",
    "\n",
    "    Returns:\n",
    "        list: List of unique document dictionaries based on file path.\n",
    "    \"\"\"\n",
    "    file_seen = set()\n",
    "    metadata_list = []\n",
    "    for doc in doc_dicts:\n",
    "        if doc[\"file_path\"] not in file_seen:\n",
    "            file_seen.add(doc[\"file_path\"])\n",
    "            metadata_list.append(doc)\n",
    "    return metadata_list\n",
    "\n",
    "def query_summaries(\n",
    "    doc_dicts: list,\n",
    "    host: str,\n",
    "    port: int,\n",
    "    model: str,\n",
    "    api_base: str = None,\n",
    "    stream: bool = False\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Generate summaries for the provided document dictionaries.\n",
    "\n",
    "    Args:\n",
    "        doc_dicts (list): List of document dictionaries.\n",
    "        host (str): Host address for the API.\n",
    "        port (int): Port number for the API.\n",
    "        model (str): AI model to use for generating summaries.\n",
    "        api_base (str, optional): Base URL for the API. Defaults to None.\n",
    "        stream (bool, optional): Whether to use streaming. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing file summaries and associated metadata.\n",
    "    \"\"\"\n",
    "    PROMPT = f\"\"\" \n",
    "    The following is a list of file contents, along with their metadata. For each file, provide a summary of the contents. The purpose of the summary is to organize files based on their content. To this end provide a concise but informative summary. Try to make the summary as specific to the file as possible. {doc_dicts}\n",
    "    \n",
    "    Return a JSON object with the following schema:\n",
    "    \n",
    "json\n",
    "    {{\n",
    "      \"files\": [\n",
    "        {{\n",
    "          \"file_path\": \"path to the file including name\",\n",
    "          \"summary\": \"summary of the content\"\n",
    "        }}\n",
    "      ]\n",
    "    }}\n",
    "    \"\"\".strip()\n",
    "\n",
    "    if not api_base:\n",
    "        api_base = f\"http://{host}:{port}\"\n",
    "\n",
    "    response = None\n",
    "    try:\n",
    "        response = completion(\n",
    "            model=model, \n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\", \n",
    "                    \"content\": \"Always return JSON. Do not include any other text or formatting characters.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": PROMPT\n",
    "                }\n",
    "            ],\n",
    "            api_base=api_base,\n",
    "            stream=stream\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"LiteLLM Error >>> \", e)\n",
    "        return {\"files\": [], \"cost\": COST_TRACKER[\"cost\"]}\n",
    "\n",
    "    if response is None:\n",
    "        return {\"files\": [], \"cost\": COST_TRACKER[\"cost\"]}\n",
    "\n",
    "    # Convert the response to a dict\n",
    "    if hasattr(response, \"to_dict\"):\n",
    "        response_dict = response.to_dict()\n",
    "    else:\n",
    "        try:\n",
    "            response_dict = json.loads(str(response))\n",
    "        except (TypeError, json.JSONDecodeError):\n",
    "            return {\"files\": [], \"cost\": COST_TRACKER[\"cost\"]}\n",
    "\n",
    "    content = response_dict.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "\n",
    "    # Attempt to parse the JSON content\n",
    "    try:\n",
    "        summaries = json.loads(content)\n",
    "    except json.JSONDecodeError:\n",
    "        summaries = {\"files\": []}\n",
    "\n",
    "    print(\"Summaries: >>>>>>>>>>> \")\n",
    "    pprint(summaries)\n",
    "\n",
    "    # If summaries is a list and we expect a dict, take the first element\n",
    "    if isinstance(summaries, list) and len(summaries) > 0 and isinstance(summaries[0], dict):\n",
    "        summaries = summaries[0]\n",
    "\n",
    "    # Add usage tokens if available\n",
    "    usage = response_dict.get(\"usage\", {})\n",
    "\n",
    "    # print(\"Usage: >>>>>>>>>>> \")\n",
    "    print(\"Usage: >>>>>>>>>>> \", usage)\n",
    "\n",
    "    if usage and isinstance(summaries, dict):\n",
    "        summaries[\"usage\"] = {\n",
    "            \"completion_tokens\": usage.get(\"completion_tokens\"),\n",
    "            \"prompt_tokens\": usage.get(\"prompt_tokens\"),\n",
    "            \"total_tokens\": usage.get(\"total_tokens\")\n",
    "        }\n",
    "\n",
    "    # Add cost from the COST_TRACKER\n",
    "    if isinstance(summaries, dict):\n",
    "        summaries[\"cost\"] = COST_TRACKER[\"cost\"]\n",
    "    else:\n",
    "        # If still not dict, fallback to a dict structure\n",
    "        summaries = {\n",
    "            \"files\": [],\n",
    "            \"cost\": COST_TRACKER[\"cost\"]\n",
    "        }\n",
    "\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of docs:  3\n",
      "[{'content': 'In the ever-evolving landscape of artificial intelligence, '\n",
      "             'Vertical Large Language Model (LLM) Agents are emerging as the '\n",
      "             'next big wave in Software as a Service (SaaS) innovations. '\n",
      "             'Imagine having an AI assistant tailored specifically to your '\n",
      "             'industry’s unique challenges and workflows, capable of '\n",
      "             'performing complex tasks in a fraction of the time it '\n",
      "             'traditionally takes. This is not a distant future but a present '\n",
      "             'reality, as evidenced by the remarkable success stories like '\n",
      "             'Case Text’s Co-Counsel. But why are these vertical LLM agents '\n",
      "             'poised to become billion-dollar opportunities, and how can your '\n",
      "             'organization leverage this transformative technology?\\n'\n",
      "             '\\n'\n",
      "             'The Moonshot Moment: A Leap Ahead of the Market\\n'\n",
      "             'Jake Heller, founder of Case Text, aptly describes their journey '\n",
      "             'with AI as the “first man on the moon” experience. Before the '\n",
      "             'public release of GPT-4, Jake and his team were already '\n",
      "             'harnessing its capabilities to revolutionize the legal industry. '\n",
      "             'This early adoption allowed them to pivot swiftly, dedicating '\n",
      "             '120 employees to develop Co-Counsel, an AI-driven legal '\n",
      "             'assistant that dramatically increased their company’s valuation '\n",
      "             'from $100 million to a $650 million acquisition by Thomson '\n",
      "             'Reuters in just two months.',\n",
      "  'creation_date': '2024-12-17',\n",
      "  'file_name': 'dsflsdflj.txt',\n",
      "  'file_path': '/home/rakesh/Downloads/sample_data/four/dsflsdflj.txt',\n",
      "  'file_size': 1233,\n",
      "  'file_type': 'text/plain',\n",
      "  'last_modified_date': '2024-12-17'},\n",
      " {'content': 'This is a test file.\\n'\n",
      "             '\\n'\n",
      "             'It has some very important information in it.\\n'\n",
      "             '\\n'\n",
      "             '\\n'\n",
      "             'Bank account: 1234567890',\n",
      "  'creation_date': '2024-12-17',\n",
      "  'file_name': 'random_file.txt',\n",
      "  'file_path': '/home/rakesh/Downloads/sample_data/four/random_file.txt',\n",
      "  'file_size': 94,\n",
      "  'file_type': 'text/plain',\n",
      "  'last_modified_date': '2024-10-24'},\n",
      " {'content': 'Next.js is an open source web development framework built on top '\n",
      "             'of Node.js that supports React-based web application features '\n",
      "             'such as server-side rendering and generating static websites.\\n'\n",
      "             '\\n'\n",
      "             'Although Next.js. officially allows us to quickly create Next.js '\n",
      "             'projects with the npx create-next-app@latest command, mature web '\n",
      "             'projects often involve user authentication, databases, payments, '\n",
      "             'and other features. Developing these features from scratch can '\n",
      "             'be challenging for Next.js newbies. There are some great open '\n",
      "             'source Next.js Starter projects that we can use to meet this '\n",
      "             'challenge and speed up our development process.\\n'\n",
      "             '\\n'\n",
      "             'Next, I’m going to introduce 8 Next.js boilerplates recommended '\n",
      "             'by the Next.js Starter Directory website.\\n'\n",
      "             '\\n'\n",
      "             '\\n'\n",
      "             'nextjs-starter-directory\\n'\n",
      "             '1. tailwind-nextjs-starter-blog\\n'\n",
      "             'This is a Next.js, Tailwind CSS blogging starter template. '\n",
      "             'Version 2 is based on Next App directory with React Server '\n",
      "             'Component and uses Contentlayer to manage markdown content.\\n'\n",
      "             '\\n'\n",
      "             'Probably the most feature-rich Next.js markdown blogging '\n",
      "             'template out there. Easily configurable and customizable. '\n",
      "             'Perfect as a replacement to existing Jekyll and Hugo individual '\n",
      "             'blogs.\\n'\n",
      "             '\\n'\n",
      "             'MIT / 8.9K+ Star/ Next.js 15\\n'\n",
      "             '\\n'\n",
      "             '\\n'\n",
      "             'tailwind-nextjs-starter-blog\\n'\n",
      "             'Tech Stack：\\n'\n",
      "             '- Framework: Next.js\\n'\n",
      "             '- UI Library: @headlessui/react\\n'\n",
      "             '- CSS Styling: Tailwind CSS\\n'\n",
      "             '- CMS: contentlayer2\\n'\n",
      "             '\\n'\n",
      "             '2. Vercel Platforms\\n'\n",
      "             'A full-stack Next.js app with multi-tenancy and custom domain '\n",
      "             'support. Built with Next.js App Router and the Vercel Domains '\n",
      "             'API.\\n'\n",
      "             '\\n'\n",
      "             'MIT / 5.7K+ Star / Next.js 14\\n'\n",
      "             '\\n'\n",
      "             '\\n'\n",
      "             'vercel-platforms\\n'\n",
      "             'Tech Stack：\\n'\n",
      "             '- Framework: Next.js\\n'\n",
      "             '- CSS Styling: Tailwind CSS\\n'\n",
      "             '- ORM: Drizzle\\n'\n",
      "             '- Auth: NextAuth.js\\n'\n",
      "             '- Editor: Novel\\n'\n",
      "             '- Charts: Tremor\\n'\n",
      "             '\\n'\n",
      "             '3. next-forge\\n'\n",
      "             'Production-grade Turborepo template for Next.js apps.\\n'\n",
      "             '\\n'\n",
      "             'MIT / 4.1K+ Star / Next.js 15\\n'\n",
      "             '\\n'\n",
      "             '\\n'\n",
      "             'next-forge\\n'\n",
      "             'Tech Stack：\\n'\n",
      "             '- Framework: Next.js\\n'\n",
      "             '- CSS Styling: Tailwind CSS\\n'\n",
      "             '- UI Library: Radix UI\\n'\n",
      "             '- ORM: Prisma\\n'\n",
      "             '- DB: Neon\\n'\n",
      "             '- Payment: Stripe\\n'\n",
      "             '\\n'\n",
      "             '4. fragments\\n'\n",
      "             'Open-source Next.js template for building apps that are fully '\n",
      "             'generated by AI.\\n'\n",
      "             '\\n'\n",
      "             'Apache-2.0 / 3.5K+ Star / Next.js 14\\n'\n",
      "             '\\n'\n",
      "             '\\n'\n",
      "             'fragments\\n'\n",
      "             'Tech Stack：\\n'\n",
      "             '- Framework: Next.js\\n'\n",
      "             '- CSS Styling: Tailwind CSS\\n'\n",
      "             '- UI Library: Radix UI\\n'\n",
      "             '- DB: Supabase\\n'\n",
      "             '\\n'\n",
      "             '5. next-shadcn-dashboard-starter\\n'\n",
      "             'Admin Dashboard Starter with Nextjs14 and shadcn ui.\\n'\n",
      "             '\\n'\n",
      "             'MIT / 2.9K+ Star / Next.js 14\\n'\n",
      "             '\\n'\n",
      "             '\\n'\n",
      "             'next-shadcn-dashboard-starter\\n'\n",
      "             'Tech Stack：\\n'\n",
      "             '- Framework: Next.js\\n'\n",
      "             '- CSS Styling: Tailwind CSS\\n'\n",
      "             '- Components: Shadcn-ui\\n'\n",
      "             '- Auth: Auth.js\\n'\n",
      "             '- State Management: Zustand\\n'\n",
      "             '\\n'\n",
      "             '6. langchain-ai/langchain-nextjs-template\\n'\n",
      "             'LangChain + Next.js starter template.\\n'\n",
      "             '\\n'\n",
      "             'MIT / 1.5K+ Star / Next.js 14\\n'\n",
      "             '\\n'\n",
      "             '\\n'\n",
      "             'langchain-nextjs-template\\n'\n",
      "             'Tech Stack：\\n'\n",
      "             '- Framework: Next.js\\n'\n",
      "             '- CSS Styling: Tailwind CSS\\n'\n",
      "             '- DB: Supbase\\n'\n",
      "             '- AI: Langchainjs\\n'\n",
      "             '\\n'\n",
      "             '7. nextjs-starter-kit\\n'\n",
      "             'The Ulimate Nextjs Starter Kit. Build your next SAAS product of '\n",
      "             'your dreams. Batteries included.\\n'\n",
      "             '\\n'\n",
      "             'MIT / 1.2K+ Star / Next.js 15',\n",
      "  'creation_date': '2024-12-17',\n",
      "  'file_name': 'shad.txt',\n",
      "  'file_path': '/home/rakesh/Downloads/sample_data/four/shad.txt',\n",
      "  'file_size': 2813,\n",
      "  'file_type': 'text/plain',\n",
      "  'last_modified_date': '2024-12-17'}]\n"
     ]
    }
   ],
   "source": [
    "path = \"/home/rakesh/Downloads/sample_data/four/\"\n",
    "doc_dicts = load_documents(path)\n",
    "print(\"# of docs: \", len(doc_dicts))  # doc_dicts\n",
    "pprint(doc_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries: >>>>>>>>>>> \n",
      "[{'file_path': '/home/rakesh/Downloads/sample_data/four/dsflsdflj.txt',\n",
      "  'summary': 'An article discussing Vertical Large Language Model (LLM) Agents '\n",
      "             'and their potential to revolutionize industries, using the '\n",
      "             'success story of Co-Counsel as an example.'},\n",
      " {'file_path': '/home/rakesh/Downloads/sample_data/four/random_file.txt',\n",
      "  'summary': 'A test file containing bank account information'},\n",
      " {'file_path': '/home/rakesh/Downloads/sample_data/four/shad.txt',\n",
      "  'summary': 'An article introducing the Next.js Starter Kit, a comprehensive '\n",
      "             'template for building SAAS products with Next.js'}]\n",
      "Usage: >>>>>>>>>>>  {'completion_tokens': 155, 'prompt_tokens': 1431, 'total_tokens': 1586, 'completion_tokens_details': None, 'prompt_tokens_details': None}\n",
      "{'cost': 0.003762,\n",
      " 'file_path': '/home/rakesh/Downloads/sample_data/four/dsflsdflj.txt',\n",
      " 'summary': 'An article discussing Vertical Large Language Model (LLM) Agents '\n",
      "            'and their potential to revolutionize industries, using the '\n",
      "            'success story of Co-Counsel as an example.',\n",
      " 'usage': {'completion_tokens': 155,\n",
      "           'prompt_tokens': 1431,\n",
      "           'total_tokens': 1586}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated cost: 0.004758\n"
     ]
    }
   ],
   "source": [
    "# Example: Using Ollama llama model\n",
    "summaries_ol = query_summaries(\n",
    "    doc_dicts=doc_dicts,\n",
    "    host=\"localhost\",\n",
    "    port=\"11434\",\n",
    "    model=\"ollama/llama3.2:latest\",\n",
    "    # model=\"ollama/smollm2:135m\",\n",
    "    api_base=\"http://localhost:11434\"\n",
    ")\n",
    "\n",
    "pprint(summaries_ol)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cost': 0.003762,\n",
      " 'file_path': '/home/rakesh/Downloads/sample_data/four/dsflsdflj.txt',\n",
      " 'summary': 'An article discussing Vertical Large Language Model (LLM) Agents '\n",
      "            'and their potential to revolutionize industries, using the '\n",
      "            'success story of Co-Counsel as an example.',\n",
      " 'usage': {'completion_tokens': 155,\n",
      "           'prompt_tokens': 1431,\n",
      "           'total_tokens': 1586}}\n"
     ]
    }
   ],
   "source": [
    "pprint(summaries_ol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summaries_ol1 = [{'file_path': '/home/rakesh/Downloads/sample_data/four/dsflsdflj.txt',\n",
    "#   'summary': 'This file discusses the emergence of Vertical Large Language '\n",
    "#              'Model (LLM) Agents as the next big wave in Software as a Service '\n",
    "#              '(SaaS) innovations, highlighting the success story of Case '\n",
    "#              \"Text's Co-Counsel, an AI-driven legal assistant that \"\n",
    "#              \"dramatically increased the company's valuation.\"},\n",
    "#  {'file_path': '/home/rakesh/Downloads/sample_data/four/random_file.txt',\n",
    "#   'summary': 'This file contains some generic test information, including a '\n",
    "#              'bank account number.'},\n",
    "#  {'file_path': '/home/rakesh/Downloads/sample_data/four/shad.txt',\n",
    "#   'summary': 'This file provides an overview of several open-source Next.js '\n",
    "#              'starter projects, including their tech stacks, features, and use '\n",
    "#              'cases, such as blogging, admin dashboards, and AI-powered '\n",
    "#              'applications.'}]\n",
    "\n",
    "\n",
    "tree_ol = create_file_tree(\n",
    "    summaries=summaries_ol,  # Pass the list of files\n",
    "    host=\"localhost\",\n",
    "    port=11434,  # Pass port as an integer\n",
    "    model=\"ollama/llama3.2:latest\",\n",
    "    # model=\"ollama/smollm2:135m\",\n",
    "    api_base=\"http://localhost:11434\"  # Ensure this matches your API's expected base URL\n",
    ")\n",
    "\n",
    "pprint(tree_ol)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated cost: 0.0061920000000000005\n",
      "Summaries: >>>>>>>>>>> \n",
      "{'files': [{'file_path': '/home/rakesh/Downloads/sample_data/four/dsflsdflj.txt',\n",
      "            'summary': 'This file discusses the emergence of Vertical Large '\n",
      "                       'Language Model (LLM) Agents as a transformative '\n",
      "                       'technology in the field of artificial intelligence. It '\n",
      "                       \"highlights the success story of Case Text's \"\n",
      "                       'Co-Counsel, an AI-driven legal assistant that '\n",
      "                       \"significantly increased the company's valuation. The \"\n",
      "                       'file explores why these vertical LLM agents are poised '\n",
      "                       'to become billion-dollar opportunities and how '\n",
      "                       'organizations can leverage this technology.'},\n",
      "           {'file_path': '/home/rakesh/Downloads/sample_data/four/random_file.txt',\n",
      "            'summary': 'This is a test file containing some important '\n",
      "                       'information, including a bank account number.'},\n",
      "           {'file_path': '/home/rakesh/Downloads/sample_data/four/shad.txt',\n",
      "            'summary': 'This file provides an overview of various Next.js '\n",
      "                       'boilerplate projects, including '\n",
      "                       'tailwind-nextjs-starter-blog, Vercel Platforms, '\n",
      "                       'next-forge, fragments, next-shadcn-dashboard-starter, '\n",
      "                       'langchain-nextjs-template, and nextjs-starter-kit. It '\n",
      "                       'highlights the key features and technology stacks of '\n",
      "                       'these Next.js starter projects, which can help '\n",
      "                       'accelerate web development using the Next.js '\n",
      "                       'framework.'}]}\n",
      "Usage: >>>>>>>>>>>  {'completion_tokens': 330, 'prompt_tokens': 1734, 'total_tokens': 2064, 'completion_tokens_details': None, 'prompt_tokens_details': {'cached_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0}\n",
      "{'cost': 0.0061920000000000005,\n",
      " 'files': [{'file_path': '/home/rakesh/Downloads/sample_data/four/dsflsdflj.txt',\n",
      "            'summary': 'This file discusses the emergence of Vertical Large '\n",
      "                       'Language Model (LLM) Agents as a transformative '\n",
      "                       'technology in the field of artificial intelligence. It '\n",
      "                       \"highlights the success story of Case Text's \"\n",
      "                       'Co-Counsel, an AI-driven legal assistant that '\n",
      "                       \"significantly increased the company's valuation. The \"\n",
      "                       'file explores why these vertical LLM agents are poised '\n",
      "                       'to become billion-dollar opportunities and how '\n",
      "                       'organizations can leverage this technology.'},\n",
      "           {'file_path': '/home/rakesh/Downloads/sample_data/four/random_file.txt',\n",
      "            'summary': 'This is a test file containing some important '\n",
      "                       'information, including a bank account number.'},\n",
      "           {'file_path': '/home/rakesh/Downloads/sample_data/four/shad.txt',\n",
      "            'summary': 'This file provides an overview of various Next.js '\n",
      "                       'boilerplate projects, including '\n",
      "                       'tailwind-nextjs-starter-blog, Vercel Platforms, '\n",
      "                       'next-forge, fragments, next-shadcn-dashboard-starter, '\n",
      "                       'langchain-nextjs-template, and nextjs-starter-kit. It '\n",
      "                       'highlights the key features and technology stacks of '\n",
      "                       'these Next.js starter projects, which can help '\n",
      "                       'accelerate web development using the Next.js '\n",
      "                       'framework.'}],\n",
      " 'usage': {'completion_tokens': 330,\n",
      "           'prompt_tokens': 1734,\n",
      "           'total_tokens': 2064}}\n"
     ]
    }
   ],
   "source": [
    "# Example: Using Anthropic Haiku (via litellm)\n",
    "# Ensure you have your ANTHROPIC_API_KEY set in the environment or .env.local\n",
    "summaries_an = query_summaries(\n",
    "    doc_dicts=doc_dicts,\n",
    "    host=\"\",         # Not needed if api_base is fully provided\n",
    "    port=\"\", \n",
    "    model=\"anthropic/claude-3-haiku-20240307\",\n",
    "    api_base=\"https://api.anthropic.com\"  # For example, if litellm expects this pattern\n",
    ")\n",
    "\n",
    "pprint(summaries_an)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cost': 0.0061920000000000005,\n",
      " 'files': [{'file_path': '/home/rakesh/Downloads/sample_data/four/dsflsdflj.txt',\n",
      "            'summary': 'This file discusses the emergence of Vertical Large '\n",
      "                       'Language Model (LLM) Agents as a transformative '\n",
      "                       'technology in the field of artificial intelligence. It '\n",
      "                       \"highlights the success story of Case Text's \"\n",
      "                       'Co-Counsel, an AI-driven legal assistant that '\n",
      "                       \"significantly increased the company's valuation. The \"\n",
      "                       'file explores why these vertical LLM agents are poised '\n",
      "                       'to become billion-dollar opportunities and how '\n",
      "                       'organizations can leverage this technology.'},\n",
      "           {'file_path': '/home/rakesh/Downloads/sample_data/four/random_file.txt',\n",
      "            'summary': 'This is a test file containing some important '\n",
      "                       'information, including a bank account number.'},\n",
      "           {'file_path': '/home/rakesh/Downloads/sample_data/four/shad.txt',\n",
      "            'summary': 'This file provides an overview of various Next.js '\n",
      "                       'boilerplate projects, including '\n",
      "                       'tailwind-nextjs-starter-blog, Vercel Platforms, '\n",
      "                       'next-forge, fragments, next-shadcn-dashboard-starter, '\n",
      "                       'langchain-nextjs-template, and nextjs-starter-kit. It '\n",
      "                       'highlights the key features and technology stacks of '\n",
      "                       'these Next.js starter projects, which can help '\n",
      "                       'accelerate web development using the Next.js '\n",
      "                       'framework.'}],\n",
      " 'usage': {'completion_tokens': 330,\n",
      "           'prompt_tokens': 1734,\n",
      "           'total_tokens': 2064}}\n"
     ]
    }
   ],
   "source": [
    "pprint(summaries_an)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated cost:[{'dst_path': '/home/rakesh/Documents/AI_Research/Vertical_LLM_Agents/co_counsel_case_study.txt',\n",
      "  'src_path': '/home/rakesh/Downloads/sample_data/four/dsflsdflj.txt'},\n",
      " {'dst_path': '/home/rakesh/Documents/Sensitive_Information/bank_account_details.txt',\n",
      "  'src_path': '/home/rakesh/Downloads/sample_data/four/random_file.txt'},\n",
      " {'dst_path': '/home/rakesh/Documents/Web_Development/NextJS_Boilerplates/overview.txt',\n",
      "  'src_path': '/home/rakesh/Downloads/sample_data/four/shad.txt'}]\n",
      " 0.003762\n"
     ]
    }
   ],
   "source": [
    "tree_an = create_file_tree(\n",
    "    summaries=summaries_an,\n",
    "    host=\"\",         # Not needed if api_base is fully provided\n",
    "    port=\"\", \n",
    "    model=\"anthropic/claude-3-haiku-20240307\",\n",
    "    api_base=\"https://api.anthropic.com\"  # For example, if litellm expects this pattern\n",
    ")\n",
    "\n",
    "pprint(tree_an)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'dst_path': '/home/rakesh/Documents/AI_Research/Vertical_LLM_Agents/co_counsel_case_study.txt',\n",
      "  'src_path': '/home/rakesh/Downloads/sample_data/four/dsflsdflj.txt'},\n",
      " {'dst_path': '/home/rakesh/Documents/Sensitive_Information/bank_account_details.txt',\n",
      "  'src_path': '/home/rakesh/Downloads/sample_data/four/random_file.txt'},\n",
      " {'dst_path': '/home/rakesh/Documents/Web_Development/NextJS_Boilerplates/overview.txt',\n",
      "  'src_path': '/home/rakesh/Downloads/sample_data/four/shad.txt'}]\n"
     ]
    }
   ],
   "source": [
    "pprint(tree_an)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
